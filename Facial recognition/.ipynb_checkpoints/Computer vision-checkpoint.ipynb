{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial recognition\n",
    "Code is divided into 4 parts:\n",
    "\n",
    "### Part 1\n",
    "User provides a sample video with a clear view of a face from different angles. The video is played back with a square around each recognized face and eye.\n",
    "\n",
    "### Part 2\n",
    "Stills (images) are taken of the face from the video and saved in grayscale to the User folder.\n",
    "\n",
    "### Part 3\n",
    "Images are labeld and used to make trainingdata.yml. This file is then used for recognizing the person from the video in part 1 in other videos.\n",
    "\n",
    "### Part 4\n",
    "A new sample video is needed for this part. The code uses the data in trainingdata.yml to recognize the person in the first video in this new video.\n",
    "If the person is recognized, the saved name will be written next to the face, and if the face is not recognized, \"Not (Person Name)\" will be written next to the face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Haar-Cascade classifiers for face and eyes\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_alt.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye_tree_eyeglasses.xml')\n",
    "\n",
    "video = (\"\") # INSERT SAMPLE VIDEO HERE\n",
    "\n",
    "# Capture images from the video\n",
    "cap = cv2.VideoCapture(video) \n",
    "while 1:\n",
    "    ret, img = cap.read()\n",
    "    \n",
    "    # Change images to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detected faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.5, 5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        # Draw rectangel around detected face\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            # Draw rectangles around detected eyes\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "    # Print how many faces software recognized\n",
    "    print (\"found \" +str(len(faces)) +\" face(s)\")\n",
    "\n",
    "    # Show video with facial recognision\n",
    "    cv2.imshow('img',img)\n",
    "\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "\n",
    "    if k == 27:\n",
    "\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give id to face in sample video\n",
    "id = input('enter user id: ')\n",
    "\n",
    "sampleN=0\n",
    "\n",
    "while 1:\n",
    "\n",
    "    ret, img = cap.read()\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        sampleN=sampleN+1\n",
    "        # Save stills from video in User folder\n",
    "        cv2.imwrite(\"User\\\\\"+str(id)+ \".\" +str(sampleN)+ \".jpg\", gray[y:y+h, x:x+w])\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        cv2.waitKey(100)\n",
    "\n",
    "    cv2.imshow('img',img)\n",
    "\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    if sampleN > 40:\n",
    "\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "rec = cv2.face.LBPHFaceRecognizer_create() \n",
    "\n",
    "path=\"User\"\n",
    "\n",
    "def getImagesWithID(path):\n",
    "    # Get images from User folder\n",
    "    imagePaths = [os.path.join(path, f) for f in os.listdir(path)]\n",
    "    faces = []\n",
    "    IDs = []\n",
    "\n",
    "    for imagePath in imagePaths:      \n",
    "\n",
    "      # Read the image and convert to grayscale\n",
    "        facesImg = Image.open(imagePath).convert('L')\n",
    "        faceNP = np.array(facesImg, 'uint8')\n",
    "\n",
    "        # Get the label of the image\n",
    "        ID = int(os.path.split(imagePath)[-1].split(\".\")[1])\n",
    "        \n",
    "        # Detect the face in the image\n",
    "        faces.append(faceNP)\n",
    "        IDs.append(ID)\n",
    "        \n",
    "        # Add detected face(s) to trainingdata.yml file\n",
    "        cv2.imshow(\"Adding faces for traning\",faceNP)\n",
    "\n",
    "        cv2.waitKey(10)\n",
    "\n",
    "    return np.array(IDs), faces\n",
    "\n",
    "# Use and save data\n",
    "Ids,faces  = getImagesWithID(path)\n",
    "rec.train(faces,Ids)\n",
    "rec.save(\"trainingdata.yml\")\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert new video for testing\n",
    "video2 = (\"\") # PATH TO VIDEO\n",
    "cap2 = cv2.VideoCapture(video2)\n",
    "# Read training data and set font for text\n",
    "rec.read(\"trainingdata.yml\")\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "while 1:\n",
    "    ret, img = cap2.read()\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.5, 5)\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        id,conf=rec.predict(gray[y:y+h,x:x+w])\n",
    "        if id==1:\n",
    "            id=\"\" #INSERT PERSON NAME HERE\n",
    "        if id !=1:\n",
    "            id=\"Not \" #INSERT PERSON NAME HERE\n",
    "        # Writes text \"PERSON NAME\" when face is recognized and \"Not PERSON NAME\" when face is not recognized\n",
    "        cv2.putText(img,id,(10,500), font, 4,(0,0,0),2,cv2.LINE_AA)\n",
    "\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "    cv2.imshow('img',img)\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
